{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from openai import OpenAI\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = \"chevalblanc/gpt-4o-mini\" # <- change this if you needed, I use this because I'm running on local ollama.\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This line of code is written in Python and uses a feature called \"yield from\" which was introduced in Python 3.3.\n",
       "\n",
       "Let's break it down:\n",
       "\n",
       "- `book.get(\"author\")`: This gets the value associated with the key `\"author\"` from a dictionary (in this case, `book`).\n",
       "\n",
       "- `{...for book in books if book.get(\"author\")}`: This is a dictionary comprehension and generator expression combined. It iterates over each item in `books` which includes an \"author\" key.\n",
       "\n",
       "    The \"if\" clause filters out any items that don't have the `\"author\"` key, so only books with authors are processed.\n",
       "    \n",
       "- `yield from ...`: When used with a generator expression (using parentheses `{...}`), it's called \"yield from\", and it takes all the items yielded by the expression instead of yielding individual elements. It yields them one after another.\n",
       "\n",
       "Putting it together, this line returns an iterator over the names of all authors in the list `books` if those users are found:\n",
       "\n",
       "\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "Example \n",
       "\n",
       "Let's assume we have a dictionary with authors\n",
       "\n",
       "python\n",
       "books = {\n",
       "    \"id1\": {\"name\": \"Book A\", \"author\": \"Author 1\"},\n",
       "    \"id2\": {\"name\": \"Book B\", \"author\": \"Author 2\"},\n",
       "    \"id3\": {},\n",
       "}\n",
       "\n",
       "We would get these values:\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "# yield from {\n",
       "#     \"Author 1\",\n",
       "#     \"Author 2\"\n",
       "# }\n",
       "\n",
       "\n",
       "Also notice how the `if` condition makes a difference here. In this case, because both of our items have an author field, we would have seen everything - even though normally all values yielded would be just `{}` and a string.\n",
       "\n",
       "python\n",
       "books = {\n",
       "    \"id1\": {\"name\": \"Book A\", \"author\": \"Author 1\"},\n",
       "    \"id2\": {},\n",
       "    \"id3\": {\"name\": \"Book B\", \"author\": \"Author 2\"},\n",
       "}\n",
       "\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "# yield from {\n",
       "#     \"Author 1\",\n",
       "#     }\n",
       "\n",
       "\n",
       "In this example, `id2` isn't there."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "stream = openai.chat.completions.create(model=MODEL_LLAMA, stream=True, messages=[\n",
    "                                            {\"role\": \"user\", \"content\": question}\n",
    "                                        ])\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
